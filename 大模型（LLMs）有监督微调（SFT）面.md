<h1> 大模型（LLMs）有监督微调（SFT）面</h1>   
<h2>一、概念篇    </h2>

<h3>从零训练一个大模型有哪几个核心步骤？      </h3>


从零开始训练一个大语言模型（LLM）是一个极其复杂的系统工程，主要可以划分为以下 **6 个核心阶段**。我们可以通过数学定义和工程实例来深入理解。

---

### 1. 数据工程（Data Engineering）

这是“炼丹”的第一步，数据的质量直接决定了模型的天花板。

* **核心动作**：
* **清洗（Cleaning）**：去除 HTML 标签、乱码、有害信息。
* **去重（De-deduplication）**：使用 MinHash 或 LSH 算法剔除互联网中高度重复的内容，防止模型“死记硬背”。
* **Tokenization**：训练分词器（如 Byte-Pair Encoding, BPE），将文本转化为数字序列。


* **生动例子**：这就像是在给大脑准备教材。如果教材里全是错别字（脏数据）或者同一篇课文印了 100 遍（重复数据），大脑就会变得迟钝。

### 2. 模型架构设计（Architecture Design）

目前主流大模型基本都采用 **Decoder-Only** 的 Transformer 架构。

* **数学核心**：核心是 **Self-Attention（自注意力机制）**。


* **关键改进**：引入旋转位置编码（RoPE）、分组查询注意力（GQA）或混合专家模型（MoE）来提升处理长文本的效率。

---

### 3. 预训练阶段（Pre-training）

这是最烧钱、最耗时的阶段，模型在此学习“世界知识”。

* **训练目标**：**Next Token Prediction**（下一词预测）。
* **数学定义**：给定前  个词，最大化第  个词出现的条件概率：


* **工程挑战**：需要大规模分布式算力（如 4xL20 或 A100 集群），利用 **ZeRO、张量并行（TP）** 等技术。

### 4. 有监督微调（SFT, Supervised Fine-Tuning）

预训练模型虽然懂得多，但不会“对话”。SFT 让模型从“背书”变成“问答”。

* **做法**：使用  的高质量指令对进行训练。
* **例子**：
* *预训练状态*：输入“今天天气不错”，模型可能续写“，我想出去玩”。
* *SFT 状态*：输入“帮我把‘今天天气不错’翻译成英文”，模型会回答“The weather is nice today.”



---

### 5. 偏好对齐（Alignment, RLHF/DPO）

让模型的回复更符合人类的价值观和偏好。

* **RLHF（强化学习）**：训练一个奖励模型（RM）给回答打分，再通过 PPO 算法更新模型。
* **DPO（直接偏好优化）**：目前的主流，直接通过对比数据（好的回复 vs 坏的回复）来训练，公式如下：



### 6. 评估与部署（Evaluation & Deployment）

* **评估**：在 MMLU、GSM8K 等基准测试集上跑分，或者进行人工评测（Side-by-side）。
* **部署优化**：通过量化（Quantization, 如 4-bit）、剪枝或蒸馏，让模型能跑在消费级显卡（如你的 L20）上。


<h3>为什么要对LLM做有监督微调（SFT）？  </h3>


简单来说，预训练（Pre-training）给了模型“知识”，而**有监督微调（SFT）给了模型“灵魂”和“规矩”**。

如果把预训练比作让学生读完图书馆里所有的书，那么 SFT 就是对他进行专门的“考前辅导”，教会他如何理解题目并按要求作答。

以下是进行 SFT 的三个核心原因：

---

### 1. 从“续写模式”切换到“对话模式”

预训练模型本质上是一个**概率续写引擎**。它的目标是根据前文预测下一个词。

* **预训练表现**：如果你问“北京的首都是哪？”，它可能会续写出“？这是一个常识问题，下一题是...”，因为它在网上看过类似的考卷。
* **SFT 的作用**：通过  的数据对，强制模型学习特定的行为模式。它教会模型：当看到问号或指令时，应该给出答案，而不是继续出题。

---

### 2. 指令遵循（Instruction Following）

在大规模预训练中，模型接触的数据大多是陈述性的（如维基百科、新闻）。模型虽然拥有知识，但并不懂得如何听从指令。

* **数学视角**：SFT 实际上是在调整模型在特定任务上的**条件概率分布** 。
* **实例**：通过 SFT，你可以训练模型专门学会“总结全文”、“代码纠错”或“文本翻译”。没有这步，模型可能知道翻译后的词是什么，但它不知道你需要它“翻译”。

---

### 3. 格式控制与领域适配

对于行业应用（如金融、医疗），我们不仅要求答案正确，还要求**格式规范**。

* **控制输出**：比如你希望模型永远以 JSON 格式输出分析结果。预训练模型无法保证这一点，但 SFT 可以通过大量 JSON 样本让模型形成“肌肉记忆”。
* **注入风格**：SFT 可以让模型学习特定的语气。比如作为客服机器人，需要语气礼貌；作为心理咨询师，需要语气温和。

---

### 4. 知识的“唤醒”与“对齐”

研究表明，SFT 过程中模型学到的“新知识”其实很少。

* **核心机制**：SFT 更多是起到一个**“指针”**的作用，它告诉模型：“把你预训练阶段学过的关于医学的那些参数调动起来，用来回答这个患者的问题。”
* **防止幻觉**：虽然不能完全消除幻觉，但 SFT 训练可以教会模型在不知道答案时回答“我不知道”，而不是胡编乱造。

---

### 💡 总结


| 阶段 | 目标 | 数据类型 | 模型表现 |
| --- | --- | --- | --- |
| **预训练** | 学习通用知识 | 海量无监督文本 | 乱说话、续写、复读机 |
| **SFT** | 学习遵循指令 | 高质量  对 | 懂礼貌、按要求作答、多轮对话 |


<h3>如何将一个基础模型训练成一个行业模型？  </h3>
将一个通用基础模型（Base Model）转化为行业模型（Domain-Specific Model），本质上是让模型完成从**“通用通才”**到**“行业专家”**的身份转变。

这通常不是靠单一的微调就能完成的，而是一套**“增量预训练 + 有监督微调 + 检索增强”**的组合拳。

---

### 1. 领域增量预训练 (Domain-Specific Continued Pre-training)

这是最底层、最扎实的步骤。目的是让模型学习行业内的**专业词汇、逻辑规律和深度知识**。

* **操作**：在数 GB 甚至 TB 级的行业原始文本（如医疗论文、法律条文、金融年报）上，继续进行“下一词预测”训练。
* **数学逻辑**：通过调整模型在特定语料上的参数 ，使得在该领域词汇序列上的似然概率  最大化。
* **生动例子**：这就像让一个刚毕业的大学生（基础模型）去律所实习，先花半年时间读完几万份卷宗，即使还没开始办案，他已经熟悉了法律术语。

---

### 2. 领域有监督微调 (Domain-Specific SFT)

在有了“底子”之后，需要教会模型如何解决具体的行业任务。

* **操作**：使用行业特有的 **{指令, 输入, 输出}** 问答对进行训练。
* **任务类型**：
* **医疗**：根据症状描述给出挂号建议。
* **金融**：根据财报数据写出摘要对比。
* **代码**：根据自然语言描述生成特定库的 API 调用。


* **关键点**：数据量不需要像预训练那么大，但**质量必须极高**，通常需要行业专家参与审核。

---

### 3. 知识对齐与偏好优化 (Domain Alignment)

行业模型往往有更严格的“价值观”和“标准答案”。

* **操作**：利用 DPO 或 RLHF，让模型学习行业内的“好坏标准”。
* **例子**：在法律咨询中，回答必须严谨客观，不能有误导性。通过给模型两个答案（一个严谨，一个随意），让模型学习选择严谨的那个。

---

### 4. 检索增强生成 (RAG - Retrieval-Augmented Generation)

对于变化极快或对准确性要求 100% 的行业知识，仅仅靠“训练”是不够的。

* **操作**：将最新的行业文档存入向量数据库。当用户提问时，先去数据库搜出相关片段，再让模型参考这些片段回答。
* **解决痛点**：解决模型的“幻觉”问题（胡编乱造）和知识滞后问题。
* **对比**：
* **微调**：像是在给学生“大脑”灌输知识。
* **RAG**：像是给学生一本“开卷考试”的参考书。



---

### 🛠️ 总结方案表

| 步骤 | 目的 | 成本 | 效果 |
| --- | --- | --- | --- |
| **增量预训练** | 学习领域术语和常识 | 高 (算力消耗大) | 提升深度，减少领域生疏感 |
| **领域 SFT** | 掌握行业任务指令 | 中 (依赖高质量标注) | 提升执行力，适配特定问答格式 |
| **RAG 架构** | 保证准确性与时效性 | 低 (工程实现为主) | 消除幻觉，提供证据溯源 |

### 💡 避坑指南

1. **灾难性遗忘**：做行业微调时，一定要混入少量通用数据，否则模型可能“学了法律忘了写代码”。
2. **数据质量 > 数量**：1,000 条高质量的专家级问答，效果远好于 10 万条爬虫抓取的垃圾问答。

<h2>二、数据篇 </h2>  

<h3>如何准备SFT阶段的训练数据？  </h3>
<h3>alpaca 格式是这么样的？</h3>
<h3>sharegpt 格式是什么样的？  </h3>
<h3>alpaca 格式和sharegpt 格式分别适合什么微调场景？  </h3>
<h3>如何自动生成指令构建SFT的训练数据？   </h3>
<h3>Self-instruct 数据生成步骤？   </h3>
三、技巧篇  
<h3>什么是灾难性遗忘？   </h3>
<h3>LM做有监督微调（SFT）变傻了怎么办？   </h3>
<h3>如何避免灾难性遗忘？   </h3>
四、对比篇  
有监督微调（SFT）和人工偏好对齐（RLHF）有何区别？  
有监督微调（SFT）适用于什么场景？  
人工偏好对齐（RLHF）适用于什么场景  
